{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd13f8ec-efbb-4ff9-8d45-e8cfc78e9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a974164-1ac9-4de6-833e-0177608b454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_csv_path = os.path.join(os.getcwd(), 'models/dynamic_gesture/data/gesture.csv')\n",
    "dynamic_classifier_path = os.path.join(os.getcwd(), 'models/dynamic_gesture/dynamic_classifier.keras')\n",
    "dynamic_tflite_path = os.path.join(os.getcwd(), 'models/dynamic_gesture/dynamic_classifier.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48641c9d-29e1-4219-8fe6-07cc62fbf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "My purpose is to take data from a csv which is a 30-frame long RECORDING of the relative positions of mediapipe \n",
    "hand landmark locations where each frame is a csv row. These rows contain coordinates and there are 21 landmarks. \n",
    "Thus, there are 42 hand landmarks (21x, 21y), and a 43rd variable representing handedness. Thus there are 43 FEATURES and \n",
    "1 y-target variable.\n",
    "\n",
    "This function takes X_data and converts it\n",
    "into blocks of 30, such that a sequence of 30 frames with 43 features is one input reference frame for an LSTM model\n",
    "\n",
    "The shape of the data is thus:\n",
    "\n",
    "gest = number of RECORDINGS\n",
    "frames = number of FRAMES per RECORDING\n",
    "features = number of FEATURES\n",
    "\n",
    "(gest, frames, features)\n",
    "or\n",
    "(gest, 30, 43)\n",
    "\n",
    "Finally, the y-target is a classification and as such the first of every 30 y_data represents the 0th row of every block, making\n",
    "the y_data array represent the classification of a corresponding X_data input block. It is encoded\n",
    "\n",
    "'''\n",
    "\n",
    "def preprocess_data(X_data, y_data, block_size=30):\n",
    "    X_data = X_data.copy()\n",
    "    y_data = y_data.copy()\n",
    "\n",
    "    # level 1: rows. Level 2: columns\n",
    "    num_rows = X_data.shape[0]\n",
    "    num_cols = X_data.shape[1]\n",
    "    # floor div, ie clip off anything that isnt a multiple of block_size (shouldnt happen due to capture method)\n",
    "    num_blocks = num_rows // block_size\n",
    "\n",
    "    # take care of the X values:\n",
    "    # we want the output to be in \"batches\" of block_size, so...\n",
    "    result_shape = (num_blocks, block_size, num_cols)\n",
    "\n",
    "    # get an index that is the consequence of finding the lowest number of blocks\n",
    "    array_ind = num_blocks * block_size\n",
    "\n",
    "    # slice & reshape\n",
    "    X_result = X_data[:array_ind].reshape(result_shape)\n",
    "    \n",
    "    y_result = y_data[::block_size]\n",
    "    \n",
    "    return X_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892facda-742a-475a-a759-5300a6ed2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(X_train, X_test, y_train, y_test, num_classes):\n",
    "\n",
    "    num_sequences = X_train.shape[0]\n",
    "    seq_length = X_train.shape[1]\n",
    "    num_features = X_train.shape[2]\n",
    "\n",
    "    input_shape = (seq_length, num_features)\n",
    "\n",
    "    # Create an LSTM model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.LSTM(64, input_shape=input_shape,return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(dynamic_classifier_path, verbose=1, save_weights_only=False)\n",
    "    # es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model \n",
    "    model.fit(X_train, \n",
    "              y_train,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[cp_callback],\n",
    "              epochs=20, \n",
    "              batch_size=1)\n",
    "\n",
    "\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10b6d40-7cd3-4b81-8376-c06aca63ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_lim represents 21 hand landmarks with X,Y coords (therefore x2) and one handedness column\n",
    "the y_target is actually first in this list so X_lim should start at 1\n",
    "'''\n",
    "\n",
    "X_lim = 1 + (21 * 2)\n",
    "X_range = range(1, X_lim + 1)\n",
    "\n",
    "X = np.loadtxt(dynamic_csv_path, delimiter=',', dtype='float32', usecols=list(X_range))\n",
    "\n",
    "y = np.loadtxt(dynamic_csv_path, delimiter=',', dtype='float32', usecols=(0)).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158907a0-625a-4615-a34b-ce2ecd706d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51c6032-2794-4c44-bc27-24ae72723281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not enough data yet for stratified kfold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# encode target variables for classification\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "ohe = ohe.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "y_train = ohe.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = ohe.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "num_classes = len(ohe.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb13e4ed-1dfc-4443-9bfe-503b669ed0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            27648     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60932 (238.02 KB)\n",
      "Trainable params: 60932 (238.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 1.3504 - accuracy: 0.3611\n",
      "Epoch 1: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 5s 37ms/step - loss: 1.3514 - accuracy: 0.3421 - val_loss: 0.8076 - val_accuracy: 0.8000\n",
      "Epoch 2/20\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 0.8018 - accuracy: 0.6000\n",
      "Epoch 2: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.8116 - accuracy: 0.5789 - val_loss: 0.7204 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.7245 - accuracy: 0.6111\n",
      "Epoch 3: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.7100 - accuracy: 0.6316 - val_loss: 0.4977 - val_accuracy: 0.8000\n",
      "Epoch 4/20\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 0.5926 - accuracy: 0.7273\n",
      "Epoch 4: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.5915 - accuracy: 0.7368 - val_loss: 0.4426 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.7133 - accuracy: 0.7500\n",
      "Epoch 5: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.6941 - accuracy: 0.7632 - val_loss: 0.4936 - val_accuracy: 0.9000\n",
      "Epoch 6/20\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 0.6219 - accuracy: 0.6765\n",
      "Epoch 6: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.5962 - accuracy: 0.7105 - val_loss: 0.4366 - val_accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.5924 - accuracy: 0.7778\n",
      "Epoch 7: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.6280 - accuracy: 0.7632 - val_loss: 0.7028 - val_accuracy: 0.6000\n",
      "Epoch 8/20\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 0.5027 - accuracy: 0.7429\n",
      "Epoch 8: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.5265 - accuracy: 0.7368 - val_loss: 0.6404 - val_accuracy: 0.6000\n",
      "Epoch 9/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.4888 - accuracy: 0.7500\n",
      "Epoch 9: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4881 - accuracy: 0.7632 - val_loss: 0.3725 - val_accuracy: 0.8000\n",
      "Epoch 10/20\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 0.6334 - accuracy: 0.7059\n",
      "Epoch 10: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.5905 - accuracy: 0.7368 - val_loss: 0.7344 - val_accuracy: 0.6000\n",
      "Epoch 11/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.6288 - accuracy: 0.6667\n",
      "Epoch 11: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.6107 - accuracy: 0.6842 - val_loss: 0.5045 - val_accuracy: 0.7000\n",
      "Epoch 12/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.4306 - accuracy: 0.8056\n",
      "Epoch 12: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4253 - accuracy: 0.8158 - val_loss: 0.4732 - val_accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.4460 - accuracy: 0.8158\n",
      "Epoch 13: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.4460 - accuracy: 0.8158 - val_loss: 0.4973 - val_accuracy: 0.7000\n",
      "Epoch 14/20\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 0.4201 - accuracy: 0.7714\n",
      "Epoch 14: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4224 - accuracy: 0.7632 - val_loss: 0.4084 - val_accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.5074 - accuracy: 0.6944\n",
      "Epoch 15: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4919 - accuracy: 0.7105 - val_loss: 0.3661 - val_accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 0.4240 - accuracy: 0.7647\n",
      "Epoch 16: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4471 - accuracy: 0.7632 - val_loss: 0.4558 - val_accuracy: 0.7000\n",
      "Epoch 17/20\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.3764 - accuracy: 0.8611\n",
      "Epoch 17: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3885 - accuracy: 0.8684 - val_loss: 0.3916 - val_accuracy: 0.8000\n",
      "Epoch 18/20\n",
      "33/38 [=========================>....] - ETA: 0s - loss: 0.3832 - accuracy: 0.8485\n",
      "Epoch 18: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3809 - accuracy: 0.8421 - val_loss: 0.5186 - val_accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "35/38 [==========================>...] - ETA: 0s - loss: 0.3536 - accuracy: 0.8286\n",
      "Epoch 19: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3544 - accuracy: 0.8158 - val_loss: 0.4588 - val_accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "34/38 [=========================>....] - ETA: 0s - loss: 0.2959 - accuracy: 0.8824\n",
      "Epoch 20: saving model to C:\\Users\\Mickey Nine\\PythonProjects\\repo\\py_projects\\hand_tracking\\models/dynamic_gesture\\dynamic_classifier.keras\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3395 - accuracy: 0.8421 - val_loss: 0.3822 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "m = generate_model(X_train, X_test, y_train, y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a42faa-f54d-4964-af2e-600fba9321f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 962ms/step - loss: 0.3822 - accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "val_loss, val_acc = m.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678c85b5-da30-49cc-aa1f-229a44a4b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "model = tf.keras.models.load_model(dynamic_classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb8638d-38e9-4f9d-815f-609eb85efbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 861ms/step\n",
      "[0.21062177 0.00387338 0.7797507  0.0057542 ]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Inference test\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6227bb-aeb9-408d-817b-c649da697a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred, report=True): \n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05f993b-c676-4093-99cc-87c7758e46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       0.50      1.00      0.67         2\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.88      0.83      0.79        10\n",
      "weighted avg       0.90      0.80      0.78        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "y_pred\n",
    "\n",
    "# we need to inverse transform y_test for the purposes of comparison (nested array problem)\n",
    "display_confusion_matrix(ohe.inverse_transform(y_test).flatten(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24df87b-92ed-4707-bbd8-8730d2d07e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(dynamic_classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90f92916-82e1-4f47-9ae6-067222b7e173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\MICKEY~1\\AppData\\Local\\Temp\\tmpm7k65q7i\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\MICKEY~1\\AppData\\Local\\Temp\\tmpm7k65q7i\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81672"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform model to tflite\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] \n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(dynamic_tflite_path, 'wb').write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d9a518b-2a80-4efb-b4c2-803e3cd6c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=dynamic_tflite_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a1e5883-e876-4093-ac4e-7baa76f1e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input/output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd3aeb58-9b90-43ee-97dd-024525d886bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Infer\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d28eabb-0940-4f40-859e-9c3700d4da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1872704  0.03388042 0.6863577  0.09249149]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363d304-e240-41af-a691-8cb94b0bd1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2e3af-3233-497a-8723-f8519342ce80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
